[졸업을 위한 DRNN](https://enhanced.kr/postviewer/1551) 시리즈의 글입니다.

[이전 글](https://enhanced.kr/postviewer/1662)에서 기계학습의 목표가 loss function이 최소가 되는 hypothesis function을 찾는 것이라고 했다. 그럼 이번에는 그 오차가 최소가 되는 hypothesis function을 **어떻게 찾을 것인지**에 대해 고민해보자.



## Differentiation

[대한민국 중등교육](https://ko.wikipedia.org/wiki/%EC%A4%91%EB%93%B1_%EA%B5%90%EC%9C%A1#%ED%95%9C%EA%B5%AD)을 잘 이수한 사람이라면(안타깝게도 일부 교육과정에서의 문과 출신자는 예외일 수 있다), 연속함수에서 최솟/최댓값을 구하기 위한 방법으로 미분을 쓸 수 있다고 배워왔을 것이다. 물론 이 케이스도 예외는 아니다. 가설 함수가 2차 이하의 간단한 함수고, 변수의 종류도 적다면 미분 후 방정식을 푸는 게 더 쉬울 수 있다. 하지만, 그런 경우는 잘 없다. 심지어 [5차 이상의 방정식을 풀어야 하거나 초월함수가 들어오게 되면 해를 구할 방법이 없을 수도 있다](https://ko.wikipedia.org/wiki/%EC%98%A4%EC%B0%A8_%EB%B0%A9%EC%A0%95%EC%8B%9D#5%EC%B0%A8%EB%B0%A9%EC%A0%95%EC%8B%9D%EC%9D%98_%EA%B7%BC). 따라서 우리는 어떤 경우에도 최솟값을 구할 수 있는 다른 방법을 찾아야 한다.



## Gradient Descent

다행스럽게도, 우리가 필요로 하는 건 완벽한 최솟값이 아니다. 주어진 데이터를 학습해서 새로 들어오는 데이터의 결과를 높은 정확도로 예측하면 되는데, 주어진 데이터를 오차 없이 완벽히 학습한다고 해서 새로 들어오는 데이터가 꼭 그 규칙을 따른다는 보장이 없기 때문에, 그냥 오차가 작기만 하면 된다.

따라서, 우리는 미분계수가 0이 되는 지점을 정확히 찾지 않고, 해당 지점을 향해 점점 가까워지는 방법만 취해도 충분하다. 이 방법이 바로 **경사 하강법(gradient descent)**이다.
$$
\theta_j := \theta_j - \alpha {{\partial} \over {\partial \theta_j}} J(\theta_0, \: \theta_1, \: \cdots , \: \theta_n)\quad\mathrm{for} \:\: j=0, \: j=1, \: \cdots , \: j=n
$$
식만 보면 복잡해보이는데, 별거 아니고 그냥 점 하나 구해서 접선 방향으로 긋겠다는 것이다. 이를 변수 종류만큼의 차원이 존재하므로 그 수만큼 편미분을 반복해서 weight 값을 업데이트하면 된다. 이 작업을 loss가 충분히 작아질 때까지 계속해서 반복한다.

여기서 헷갈리면 안된다. loss function을 편미분할 때는 $x$가 아닌 $\theta$로 편미분해야 한다. $x$는 입력값이고, 우리가 구해야 하는 건 hypothesis function이다. hypothesis function을 구한다는 건 각 단항의 계수를 구하는 것이므로 결국 $\theta$를 구해야 한다. 따라서 $\theta$로 편미분해야 한다. 헷갈리지 말자.

$:=$는 대입연산자다. 수많은 프로그래밍 언어에서 쓰는 `=`와 정확히 같은 뜻으로, 새로운 값으로 업데이트하라는 뜻이다.

$\alpha$는 **learning rate**이다. 한 번에 하강하는 보폭을 결정짓는데, 그냥 코드 짜는 사람이 적당한 값 넣어보면서 실험적으로 구해야 하는 값이다. 이 값이 너무 작으면 최솟값을 구하는데 몇백년이 걸리고, 너무 크면 아예 최솟값을 못 구하고 발산해버린다. 이유에 대해서는 이차함수 그래프 하나 그려놓고 곰곰이 생각해보길 바란다.

#### Optimizers

* Adam
* 