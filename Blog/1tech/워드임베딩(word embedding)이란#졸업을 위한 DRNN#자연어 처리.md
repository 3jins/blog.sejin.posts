[졸업을 위한 DRNN](https://enhanced.kr/postviewer/1551) 시리즈의 글입니다.

RNN은 sequential data를 학습하는데에 최적화되어 있다. Sequential data란, n번째 데이터가 n-1번째 데이터에 종속성을 갖는 데이터를 말하며, 그 예시로는 소리, 뇌파, 언어, 게임행동로그 등이 있겠다.

그 중 특별한 입력장치 없이 쉽게 구할 수 있는 데이터는 대부분 언어데이터다. 하지만, 언어데이터는 **word embedding**이라는 전처리과정을 필요로 한다는 장벽이 있다. 다른 주제로 졸업작품을 진행하려고 찾아보다가 워낙 데이터셋 구하기가 어려워서 그냥 진입장벽 깨버리고 언어데이터를 쓰기로 했다. 이를 위해 워드 임베딩이 무엇인지에 대해 알아보도록 하자.



## Word Embedding

워드 임베딩이란, 단어를 수치(벡터)로 표현하는 작업이다. [뉴럴 네트워크](https://enhanced.kr/postviewer/1732)를 공부해본 사람은 알겠지만 뉴럴 네트워크의 각 노드는 weight를 곱할 수 있게끔 number vector를 입력으로 받는다. 자연어의 각 단어는 number vector가 아니기 때문에 뉴럴 네트워크에 입력으로 전달할 수가 없다. 따라서 침대에 사람의 키를 맞춰버리는 프로크루스테스 마냥 신경망에 자연어의 형식을 끼워맞춰야 하는데 이 과정이 바로 워드 임베딩(word embedding)이다.

| <img src="https://raw.githubusercontent.com/42deSix/Images/master/플루타크짤/프로크루스테스의침대2.png" width="50%"/> |
| :----------------------------------------------------------: |
|         침대 길이에 사람 키를 맞추는 프로크루스테스          |



## Feature Representation

Feature representation이란 주어진 자연어 문장에서 등장하는 단어를 벡터로 매핑시키는 것을 의미한다. 원래 신경망에서 입력벡터는 특징값들의 집합이었기 때문에(예를 들어 집값) 이렇게 쓰는 것 같다.

### sparse representation

{'고양이', '사과', '애옹이'}의 세 단어를 임베딩시킨다고 하자. 생각할 수 있는 가장 쉬운 방법은 각 단어를 one-hot encoding 벡터로 매핑시키는 방법이다. 예를 들어, '고양이'는 [1, 0, 0]으로, '사과'는 [0, 1, 0]으로, '애옹이'는 [0, 0, 1]으로 매핑시킬 수 있다. 한 언어의 이 방법으로 모든 단어를 임베딩시키려면 사전 데이터 하나 가져와서 죄다 이렇게 매핑시키면 된다.

하지만 이 방법에는 문제가 있다. 첫 번째로, 단어 수만큼의 차원이 만들어져야 한다. 이로 인해 나중의 학습과정에서 [차원의 저주](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_learning)에 빠질 확률이 높아진다. 둘째로, 단어간의 상관관계가 전혀 없다. '고양이'와 '애옹이'는 동의어지만 둘 사이의 상관관계를 유추할 만한 요소가 임베딩된 벡터에서는 전혀 보이지 않는다.

### dense representation

이를 보완하기 위해 나온 방식이 dense representation이다. 이 방법에서는 차원 수를 적당한 값 n으로 고정해서 모든 단어가 n차원짜리 벡터로 매핑된다(n의 범위는 보통 20~200 정도로 잡는다고 한다). 그럼 각 단어는 n차원 좌표상에서 하나씩의 점을 갖게 되며, 의미가 가까운 단어일수록 점들간의 거리도 가깝게 잡는다. 

이 방식을 구현한 모델은 Word2Vec, GloVe, FastText 등 여러가지가 있다고 한다. 그 중 word2vec에 대해 다음 글에서 다뤄보도록 하자. ㅎㅎ
